"""
OPTIMIZED MODEL INFERENCE
–ë—ã—Å—Ç—Ä—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –±–∞—Ç—á–∏–Ω–≥–æ–º
"""

import logging
import pickle
from pathlib import Path
from typing import List, Dict, Tuple
import time

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# –ò–µ—Ä–∞—Ä—Ö–∏—è —Ç–µ–º –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π
PARENTS = {
    "–ö–∞—Ä—Ç—ã": {"–ö–æ–º–∏—Å—Å–∏–∏ –∏ —Ç–∞—Ä–∏—Ñ—ã", "–ü–µ—Ä–µ–≤–æ–¥—ã –∏ –ø–ª–∞—Ç–µ–∂–∏", "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "–ö—ç—à–±–µ–∫ –∏ –±–æ–Ω—É—Å—ã"}
}

# –õ–µ–∫—Å–∏–∫–æ–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–ø–∞–Ω–æ–≤
TOPIC_LEX = {
    "–û—Ç–¥–µ–ª–µ–Ω–∏—è": ["–æ—á–µ—Ä–µ–¥", "–∂–¥–∞–ª", "—Ç–∞–ª–æ–Ω", "–æ–ø–µ—Ä–∞—Ç–æ—Ä", "–æ–∫–Ω–æ"],
    "–ö–∞—Ä—Ç—ã": ["–∫–æ–º–∏—Å—Å", "—Å–Ω—è—Ç", "–Ω–µ –ø—Ä–æ—Ö–æ–¥", "–∫–∞—Ä—Ç", "–ø–ª–∞—Å—Ç–∏–∫"],
    "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å": ["–±–ª–æ–∫–∏—Ä", "—Ñ—Ä–æ–¥", "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥", "–±–µ–∑–æ–ø–∞—Å–Ω"],
    "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ": ["–∑–∞–≤–∏—Å", "–ª–∞–≥", "–æ–±–Ω–æ–≤–ª–µ–Ω", "–∫—Ä–∞—à", "–ø—Ä–∏–ª–æ–∂–µ–Ω", "–∞–ø–ø"],
    "–î–µ–ø–æ–∑–∏—Ç—ã –∏ –≤–∫–ª–∞–¥—ã": ["–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç", "–ø—Ä–æ—Ü–µ–Ω—Ç", "–¥–æ—Å—Ä–æ—á–Ω"],
    "–ü–µ—Ä–µ–≤–æ–¥—ã –∏ –ø–ª–∞—Ç–µ–∂–∏": ["–ø–µ—Ä–µ–≤–æ–¥", "–ø–ª–∞—Ç–µ–∂", "–∫–æ–º–º—É–Ω–∞–ª", "—à—Ç—Ä–∞—Ñ"],
    "–°–ª—É–∂–±–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏": ["–ø–æ–¥–¥–µ—Ä–∂", "–≥–æ—Ä—è—á–∞—è –ª–∏–Ω–∏—è", "–∫–æ–ª–ª", "–æ–ø–µ—Ä–∞—Ç–æ—Ä"],
    "–ö—Ä–µ–¥–∏—Ç—ã –∏ –∏–ø–æ—Ç–µ–∫–∞": ["–∫—Ä–µ–¥–∏—Ç", "–∏–ø–æ—Ç–µ–∫", "–∑–∞—ë–º", "–¥–æ–ª–≥"],
    "–ö—ç—à–±–µ–∫ –∏ –±–æ–Ω—É—Å—ã": ["–∫—ç—à–±–µ–∫", "–∫–µ—à–±—ç–∫", "–±–æ–Ω—É—Å", "–º–∏–ª–ª"],
    "–ë–∞–Ω–∫–æ–º–∞—Ç—ã": ["–±–∞–Ω–∫–æ–º–∞—Ç", "–Ω–∞–ª–∏—á–Ω", "—Å–Ω—è—Ç—å"]
}

# –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã –¥–ª—è –ª–æ–≥–∏—Ç-–±–∞–π–∞—Å–∞
NEG_HINTS = {
    "–ö–∞—Ä—Ç—ã": ["–∫–æ–º–∏—Å—Å", "—Å–Ω—è—Ç", "–Ω–∞–ª–∏—á", "–Ω–µ —Ä–∞–±–æ—Ç–∞"],
    "–û—Ç–¥–µ–ª–µ–Ω–∏—è": ["–æ—á–µ—Ä–µ–¥", "–∂–¥–∞–ª", "–¥–æ–ª–≥–æ"],
    "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ": ["–∑–∞–≤–∏—Å", "–æ—à–∏–±–∫", "–ª–∞–≥", "–≥–ª—é—á"],
    "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å": ["–±–ª–æ–∫–∏—Ä", "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥", "–∑–∞–±–ª–æ–∫"],
    "–°–ª—É–∂–±–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏": ["–Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç", "–Ω–µ–¥–æ–∑–≤–æ–Ω", "–∏–≥–Ω–æ—Ä"]
}


class OptimizedReviewClassifier:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ—Ç–∑—ã–≤–æ–≤ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, models_dir: str = "./trained_models"):
        """
        Args:
            models_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        """
        self.models_dir = Path(models_dir)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OptimizedReviewClassifier...")
        logger.info(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        self._load_models()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.stats = {
            'total_predictions': 0,
            'total_time': 0.0,
        }
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        
        topic_model_path = self.models_dir / "topic_classifier"
        sentiment_model_path = self.models_dir / "sentiment_classifier"
        
        if not topic_model_path.exists() or not sentiment_model_path.exists():
            raise FileNotFoundError(
                f"–ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {self.models_dir}. "
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ train_optimized.py –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."
            )
        
        # Topic Classifier
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ topic –º–æ–¥–µ–ª–∏ –∏–∑ {topic_model_path}")
        self.topic_tokenizer = AutoTokenizer.from_pretrained(str(topic_model_path))
        self.topic_model = AutoModelForSequenceClassification.from_pretrained(str(topic_model_path))
        self.topic_model.to(self.device)
        self.topic_model.eval()
        
        # CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.device.type == "cpu":
            torch.set_num_threads(max(1, torch.get_num_threads()))
            if hasattr(torch.backends, 'mkldnn'):
                torch.backends.mkldnn.enabled = True
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è CPU (—É—Å–∫–æ—Ä—è–µ—Ç –≤ 1.5-2x)
            try:
                from torch.quantization import quantize_dynamic
                self.topic_model = quantize_dynamic(
                    self.topic_model, {torch.nn.Linear}, dtype=torch.qint8
                )
                logger.info("‚úÖ Topic model: –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è")
            except Exception as e:
                logger.warning(f"–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è topic model –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ MultiLabelBinarizer
        mlb_path = topic_model_path / "mlb.pkl"
        with open(mlb_path, 'rb') as f:
            self.mlb = pickle.load(f)
        
        self.topics = list(self.mlb.classes_)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.topics)} —Ç–µ–º")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ per-class
        thr_path = topic_model_path / "thresholds.json"
        self.class_thresholds = None
        if thr_path.exists():
            import json
            obj = json.load(open(thr_path, "r", encoding="utf-8"))
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–æ—Ä—è–¥–æ–∫ classes —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å self.topics
            if obj.get("classes") == self.topics:
                self.class_thresholds = obj["thresholds"]
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è {len(self.class_thresholds)} –∫–ª–∞—Å—Å–æ–≤")
            else:
                logger.warning("‚ö†Ô∏è  –ü–æ—Ä—è–¥–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –≤ thresholds.json –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –º–æ–¥–µ–ª—å—é")
        else:
            logger.warning("‚ö†Ô∏è  –ö–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è threshold=0.4")
        
        # Sentiment Classifier
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ sentiment –º–æ–¥–µ–ª–∏ –∏–∑ {sentiment_model_path}")
        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(str(sentiment_model_path))
        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(str(sentiment_model_path))
        self.sentiment_model.to(self.device)
        self.sentiment_model.eval()
        
        # CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è sentiment
        if self.device.type == "cpu":
            try:
                from torch.quantization import quantize_dynamic
                self.sentiment_model = quantize_dynamic(
                    self.sentiment_model, {torch.nn.Linear}, dtype=torch.qint8
                )
                logger.info("‚úÖ Sentiment model: –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è")
            except Exception as e:
                logger.warning(f"–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è sentiment model –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
        
        self.sentiment_labels = {
            0: "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ",
            1: "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ",
            2: "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ"
        }
        
        logger.info("‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    def get_available_topics(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ–º"""
        return self.topics
    
    def _suppress_parents(self, topics: List[str]) -> List[str]:
        """–ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Ç–µ–º –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –¥–æ—á–µ—Ä–Ω–∏—Ö"""
        ts = set(topics)
        for parent, childs in PARENTS.items():
            if parent in ts and any(ch in ts for ch in childs):
                ts.discard(parent)
        return list(ts)
    
    def _extract_relevant_span(self, text: str, topic: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —Ç–µ–º—ã"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sents = [s.strip() for s in text.replace("‚Äî", ".").replace("‚Äì", ".").split(".") if s.strip()]
        keys = TOPIC_LEX.get(topic, [])
        
        # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        for s in sents:
            low = s.lower()
            if any(k in low for k in keys):
                return s
        
        # Fallback: –ø–µ—Ä–≤—ã–µ 220 —Å–∏–º–≤–æ–ª–æ–≤
        return text[:220]
    
    def predict_topics_batch(self, texts: List[str], threshold: float = 0.4) -> List[List[str]]:
        """
        BATCH –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (–±—ã—Å—Ç—Ä–µ–µ!)
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤
            threshold: –ü–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è multilabel (0-1)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ç–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        if not texts:
            return []
        
        # –ë–∞—Ç—á —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º padding (–±—ã—Å—Ç—Ä–µ–µ)
        inputs = self.topic_tokenizer(
            texts,
            return_tensors="pt",
            truncation=True,
            max_length=128,
            padding="longest"  # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ –≤ –±–∞—Ç—á–µ
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # –ë–∞—Ç—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å inference_mode (–±—ã—Å—Ç—Ä–µ–µ)
        with torch.inference_mode():
            outputs = self.topic_model(**inputs)
            probabilities = torch.sigmoid(outputs.logits).cpu().numpy()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å per-class thresholds
        results = []
        for i in range(len(texts)):
            if self.class_thresholds:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
                import numpy as np
                mask = probabilities[i] > np.array(self.class_thresholds)
                predicted_topics = [self.topics[j] for j, ok in enumerate(mask) if ok]
            else:
                # Fallback –Ω–∞ –µ–¥–∏–Ω—ã–π threshold
                mask = probabilities[i] > threshold
                predicted_topics = [self.topics[j] for j, val in enumerate(mask) if val]
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –±–µ—Ä—ë–º —Ç–æ–ø-1 –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            if not predicted_topics:
                top_idx = int(probabilities[i].argmax())
                predicted_topics = [self.topics[top_idx]]
            
            # –ü–æ–¥–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ —Ç–µ–º—ã
            predicted_topics = self._suppress_parents(predicted_topics)
            
            results.append(predicted_topics)
        
        return results
    
    def predict_topics(self, text: str, threshold: float = 0.4) -> List[str]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        return self.predict_topics_batch([text], threshold)[0]
    
    def predict_sentiment_batch(self, texts: List[str], topics: List[str]) -> List[str]:
        """
        BATCH –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–∞—Ä (—Ç–µ–∫—Å—Ç, —Ç–µ–º–∞)
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ (–º–æ–∂–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è)
            topics: –°–ø–∏—Å–æ–∫ —Ç–µ–º (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—Å—Ç–∞–º –ø–æ –∏–Ω–¥–µ–∫—Å—É)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã
        """
        if not texts or not topics or len(texts) != len(topics):
            return []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–ø–∞–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
        spans = [self._extract_relevant_span(text, topic) for text, topic in zip(texts, topics)]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –±–∞—Ç—á —Å —Ç–µ–º–∞–º–∏
        augmented_texts = [f"[{topic}] {span}" for topic, span in zip(topics, spans)]
        
        # –ë–∞—Ç—á —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º padding
        inputs = self.sentiment_tokenizer(
            augmented_texts,
            return_tensors="pt",
            truncation=True,
            max_length=128,
            padding="longest"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # –ë–∞—Ç—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏
        with torch.inference_mode():
            outputs = self.sentiment_model(**inputs)
            logits = outputs.logits
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥–∏—Ç-–±–∞–π–∞—Å –ø–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–∏–≥–≥–µ—Ä–∞–º
            biases = []
            for span, topic in zip(spans, topics):
                low = span.lower()
                if any(k in low for k in NEG_HINTS.get(topic, [])):
                    biases.append([+0.25, 0.0, -0.25])  # [neg, neu, pos]
                else:
                    biases.append([0.0, 0.0, 0.0])
            
            biases_tensor = torch.tensor(biases, dtype=torch.float).to(logits.device)
            logits = logits + biases_tensor
            
            # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ (—Å–Ω–∏–∂–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
            logits = logits / 1.4
            
            # Softmax –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            probs = torch.softmax(logits, dim=-1)
            conf, sentiment_ids = probs.max(dim=-1)
            
            # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç –ø—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            sentiment_ids = torch.where(
                conf < 0.55,
                torch.tensor(1, device=sentiment_ids.device),  # 1 = –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ
                sentiment_ids
            )
            
            sentiment_ids = sentiment_ids.cpu().numpy()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å—Ç—Ä–æ–∫–∏
        return [self.sentiment_labels[int(sid)] for sid in sentiment_ids]
    
    def predict_sentiment(self, text: str, topic: str) -> str:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–¥–Ω–æ–π –ø–∞—Ä—ã (—Ç–µ–∫—Å—Ç, —Ç–µ–º–∞)"""
        return self.predict_sentiment_batch([text], [topic])[0]
    
    def predict_batch(self, texts: List[str], threshold: float = 0.4) -> List[Dict]:
        """
        –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –±–∞—Ç—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–ø–∏—Å–∫–∞ –æ—Ç–∑—ã–≤–æ–≤
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞—Ç—á–∏–Ω–≥ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤
            threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å topics –∏ sentiments –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–∑—ã–≤–∞
        """
        if not texts:
            return []
        
        start_time = time.time()
        
        # 1. –ë–∞—Ç—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º –¥–ª—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ä–∞–∑—É
        all_topics = self.predict_topics_batch(texts, threshold)
        
        # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞—Ç—á–∞ –¥–ª—è sentiment –∞–Ω–∞–ª–∏–∑–∞
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞—Ä—ã (—Ç–µ–∫—Å—Ç, —Ç–µ–º–∞) –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –±–∞—Ç—á
        sentiment_texts = []
        sentiment_topics = []
        review_indices = []  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∫ –∫–∞–∫–æ–º—É –æ—Ç–∑—ã–≤—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        
        for idx, (text, topics) in enumerate(zip(texts, all_topics)):
            for topic in topics:
                sentiment_texts.append(text)
                sentiment_topics.append(topic)
                review_indices.append(idx)
        
        # 3. –ë–∞—Ç—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä —Å—Ä–∞–∑—É
        all_sentiments = self.predict_sentiment_batch(sentiment_texts, sentiment_topics)
        
        # 4. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –æ—Ç–∑—ã–≤–∞–º
        results = [{"topics": [], "sentiments": []} for _ in range(len(texts))]
        
        for review_idx, topic, sentiment in zip(review_indices, sentiment_topics, all_sentiments):
            results[review_idx]["topics"].append(topic)
            results[review_idx]["sentiments"].append(sentiment)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        elapsed = time.time() - start_time
        self.stats['total_predictions'] += len(texts)
        self.stats['total_time'] += elapsed
        
        avg_time = elapsed / len(texts) if texts else 0
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(texts)} –æ—Ç–∑—ã–≤–æ–≤ –∑–∞ {elapsed:.2f}—Å ({avg_time:.3f}—Å/–æ—Ç–∑—ã–≤)")
        
        return results
    
    def predict(self, texts: List[str]) -> List[Dict]:
        """–ê–ª–∏–∞—Å –¥–ª—è predict_batch (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å API)"""
        return self.predict_batch(texts)
    
    def predict_single(self, text: str) -> Dict:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å topics –∏ sentiments
        """
        return self.predict_batch([text])[0]
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏"""
        if self.stats['total_predictions'] > 0:
            avg_time = self.stats['total_time'] / self.stats['total_predictions']
        else:
            avg_time = 0
        
        return {
            'total_predictions': self.stats['total_predictions'],
            'total_time': self.stats['total_time'],
            'avg_time_per_review': avg_time,
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
_classifier_instance = None


def get_classifier() -> OptimizedReviewClassifier:
    """Singleton –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
    global _classifier_instance
    if _classifier_instance is None:
        _classifier_instance = OptimizedReviewClassifier()
    return _classifier_instance


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    logger.info("=" * 60)
    logger.info("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô –ú–û–î–ï–õ–ò")
    logger.info("=" * 60)
    
    classifier = OptimizedReviewClassifier()
    
    test_texts = [
        "–ö–∞—Ä—Ç—ã –≥–æ–≤–Ω–æ –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é, –æ—Ç–¥–µ–ª–µ–Ω–∏—è —Ö–æ—Ä–æ—à–∏–µ, –ø–µ—Ä—Å–æ–Ω–∞–ª —Ç–∞–∫ —Å–µ–±–µ",
        "–û—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏, –Ω–æ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —á–∞—Å—Ç–æ –∑–∞–≤–∏—Å–∞–µ—Ç",
        "–ö—ç—à–±–µ–∫ –æ—Ç–ª–∏—á–Ω—ã–π, –≤—Å–µ–º —Å–æ–≤–µ—Ç—É—é!",
        "–£–∂–∞—Å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞, –Ω–∏–∫—Ç–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç",
        "–•–æ—Ä–æ—à–∏–π –±–∞–Ω–∫, –Ω–æ –∫–æ–º–∏—Å—Å–∏–∏ –≤—ã—Å–æ–∫–∏–µ –∏ —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏ –Ω–∞–≤—è–∑—ã–≤–∞—é—Ç",
        "–ü—Ä–µ–º–∏—É–º –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –Ω–∞ –≤—ã—Å–æ—Ç–µ, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫ —É–¥–æ–±–Ω—ã–π",
        "–ö—Ä–µ–¥–∏—Ç –≤–∑—è–ª - –ø—Ä–æ—Ü–µ–Ω—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π, –Ω–æ –∏–ø–æ—Ç–µ–∫–∞ –¥–æ—Ä–æ–≥–∞—è",
        "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ, –Ω–æ –¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ —Ç–æ—Ä–º–æ–∑–∏—Ç",
    ]
    
    logger.info(f"\nüß™ –¢–µ—Å—Ç 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(test_texts)} –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É")
    start = time.time()
    for text in test_texts:
        result = classifier.predict_single(text)
        logger.info(f"\n–¢–µ–∫—Å—Ç: {text[:60]}...")
        logger.info(f"–¢–µ–º—ã: {result['topics']}")
        logger.info(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {result['sentiments']}")
    elapsed_single = time.time() - start
    logger.info(f"\n‚è±Ô∏è  –í—Ä–µ–º—è (–ø–æ –æ–¥–Ω–æ–º—É): {elapsed_single:.2f}—Å ({elapsed_single/len(test_texts):.3f}—Å/–æ—Ç–∑—ã–≤)")
    
    logger.info(f"\nüöÄ –¢–µ—Å—Ç 2: –ë–∞—Ç—á –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(test_texts)} –æ—Ç–∑—ã–≤–æ–≤")
    start = time.time()
    results = classifier.predict_batch(test_texts)
    elapsed_batch = time.time() - start
    logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è (–±–∞—Ç—á): {elapsed_batch:.2f}—Å ({elapsed_batch/len(test_texts):.3f}—Å/–æ—Ç–∑—ã–≤)")
    logger.info(f"‚ö° –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {elapsed_single/elapsed_batch:.1f}x")
    
    for text, result in zip(test_texts, results):
        logger.info(f"\n–¢–µ–∫—Å—Ç: {text[:60]}...")
        logger.info(f"–¢–µ–º—ã: {result['topics']}")
        logger.info(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {result['sentiments']}")
    
    logger.info("\n" + "=" * 60)
    logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    stats = classifier.get_stats()
    logger.info(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_predictions']} –æ—Ç–∑—ã–≤–æ–≤")
    logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {stats['avg_time_per_review']:.3f}—Å/–æ—Ç–∑—ã–≤")
    logger.info("=" * 60)
