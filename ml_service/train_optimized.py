#!/usr/bin/env python3
"""
OPTIMIZED TRAINING SCRIPT
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""

import json
import logging
import pickle
from pathlib import Path
from typing import List, Tuple, Dict
import warnings

import torch
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report, f1_score, hamming_loss, precision_recall_fscore_support
from sklearn.utils.class_weight import compute_class_weight
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from torch.utils.data import Dataset
from torch.nn import BCEWithLogitsLoss
import torch.nn.functional as F
from collections import Counter

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
# –í–ê–ñ–ù–û: –ø–æ—Ä—è–¥–æ–∫ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ª–æ–≥–∏—Ç–æ–≤ –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ø–æ—Ä–æ–≥–æ–≤
EXTENDED_TOPICS = [
    "–ö–∞—Ä—Ç—ã",
    "–ö—ç—à–±–µ–∫ –∏ –±–æ–Ω—É—Å—ã",
    "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ",
    "–û—Ç–¥–µ–ª–µ–Ω–∏—è",
    "–ü–µ—Ä–µ–≤–æ–¥—ã –∏ –ø–ª–∞—Ç–µ–∂–∏",
    "–°–ª—É–∂–±–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏",
    "–î–µ–ø–æ–∑–∏—Ç—ã –∏ –≤–∫–ª–∞–¥—ã",
    "–ö—Ä–µ–¥–∏—Ç—ã –∏ –∏–ø–æ—Ç–µ–∫–∞",
    "–ü—Ä–µ–º–∏—É–º-–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ",
    "–ö–æ–º–∏—Å—Å–∏–∏ –∏ —Ç–∞—Ä–∏—Ñ—ã",
    "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "–î–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ",
    "–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –∏ —Å–µ—Ä–≤–∏—Å",
    "–ë–∞–Ω–∫–æ–º–∞—Ç—ã",
    "–°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ",
    "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏",
]

# –ö–∞—Å—Ç–æ–º–Ω—ã–π Trainer —Å pos_weight –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º —Ç–µ–º
class BCEWithPosWeightTrainer(Trainer):
    def __init__(self, *args, pos_weight=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.pos_weight = pos_weight

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        loss_fct = BCEWithLogitsLoss(
            pos_weight=self.pos_weight.to(logits.device) if self.pos_weight is not None else None
        )
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

# –ö–∞—Å—Ç–æ–º–Ω—ã–π Trainer —Å class_weights –¥–ª—è sentiment
class WeightedCELossTrainer(Trainer):
    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        loss = F.cross_entropy(
            logits, labels,
            weight=self.class_weights.to(logits.device) if self.class_weights is not None else None
        )
        return (loss, outputs) if return_outputs else loss


class ReviewDataset(Dataset):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, texts, labels, tokenizer, max_length=128):  # –£–º–µ–Ω—å—à–∏–ª–∏ max_length
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.float)
        }


def load_data() -> Tuple[List[str], List[List[str]], List[List[str]]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (id, topics, sentiments)
    reviews_path = Path('../backend/GoBackend/data/reviews.json')
    with open(reviews_path, 'r', encoding='utf-8') as f:
        labeled_data = json.load(f)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤ (id, text, ...)
    site_reviews_path = Path('../backend/GoBackend/data/siteReviews.json')
    with open(site_reviews_path, 'r', encoding='utf-8') as f:
        site_reviews_data = json.load(f)
    
    # –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ id -> text
    if isinstance(site_reviews_data, dict) and 'reviews' in site_reviews_data:
        id_to_text = {review['id']: review['text'] for review in site_reviews_data['reviews']}
    else:
        id_to_text = {review['id']: review['text'] for review in site_reviews_data}
    
    texts = []
    topics_list = []
    sentiments_list = []
    
    # –ú–µ—Ä–¥–∂–∏–º –¥–∞–Ω–Ω—ã–µ
    for review in labeled_data:
        if 'topics' in review and 'sentiments' in review and review['id'] in id_to_text:
            text = id_to_text[review['id']]
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            text = text.strip()
            if text:
                texts.append(text)
                topics_list.append(review['topics'])
                sentiments_list.append(review['sentiments'])
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
    
    return texts, topics_list, sentiments_list


def analyze_data_distribution(topics_list: List[List[str]], sentiments_list: List[List[str]]):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    
    logger.info("\n" + "=" * 60)
    logger.info("–ê–ù–ê–õ–ò–ó –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–•")
    logger.info("=" * 60)
    
    # –ü–æ–¥—Å—á–µ—Ç —Ç–µ–º
    all_topics = [topic for topics in topics_list for topic in topics]
    topic_counts = Counter(all_topics)
    
    logger.info("\n–¢–æ–ø-15 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–µ–º:")
    for topic, count in topic_counts.most_common(15):
        logger.info(f"  {topic}: {count}")
    
    # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π
    all_sentiments = [sent for sents in sentiments_list for sent in sents]
    sentiment_counts = Counter(all_sentiments)
    
    logger.info("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π:")
    for sentiment, count in sentiment_counts.items():
        percentage = count / len(all_sentiments) * 100
        logger.info(f"  {sentiment}: {count} ({percentage:.1f}%)")
    
    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    avg_topics = np.mean([len(topics) for topics in topics_list])
    logger.info(f"\n–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –Ω–∞ –æ—Ç–∑—ã–≤: {avg_topics:.2f}")
    
    logger.info("=" * 60 + "\n")


def augment_data(texts: List[str], topics_list: List[List[str]], 
                  sentiments_list: List[List[str]]) -> Tuple[List[str], List[List[str]], List[List[str]]]:
    """
    –ü—Ä–æ—Å—Ç–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
    –î—É–±–ª–∏—Ä—É–µ–º –æ—Ç–∑—ã–≤—ã —Å —Ä–µ–¥–∫–∏–º–∏ —Ç–µ–º–∞–º–∏
    """
    
    # –ü–æ–¥—Å—á–µ—Ç —Ç–µ–º
    all_topics = [topic for topics in topics_list for topic in topics]
    topic_counts = Counter(all_topics)
    
    # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–¥–∫–∏–µ —Ç–µ–º—ã (< 10% –æ—Ç —Å–∞–º–æ–π —á–∞—Å—Ç–æ–π)
    max_count = topic_counts.most_common(1)[0][1]
    rare_topics = {topic for topic, count in topic_counts.items() if count < max_count * 0.1}
    
    if not rare_topics:
        logger.info("–†–µ–¥–∫–∏—Ö —Ç–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        return texts, topics_list, sentiments_list
    
    logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(rare_topics)} —Ä–µ–¥–∫–∏—Ö —Ç–µ–º: {rare_topics}")
    
    augmented_texts = list(texts)
    augmented_topics = list(topics_list)
    augmented_sentiments = list(sentiments_list)
    
    # –î—É–±–ª–∏—Ä—É–µ–º –æ—Ç–∑—ã–≤—ã —Å —Ä–µ–¥–∫–∏–º–∏ —Ç–µ–º–∞–º–∏
    for i, topics in enumerate(topics_list):
        if any(topic in rare_topics for topic in topics):
            augmented_texts.append(texts[i])
            augmented_topics.append(topics_list[i])
            augmented_sentiments.append(sentiments_list[i])
    
    logger.info(f"–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {len(texts)} ‚Üí {len(augmented_texts)} –æ—Ç–∑—ã–≤–æ–≤")
    
    return augmented_texts, augmented_topics, augmented_sentiments


def compute_metrics_multilabel(pred):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è multilabel –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    labels = pred.label_ids
    preds = pred.predictions
    preds = (preds > 0.5).astype(int)
    
    f1_micro = f1_score(labels, preds, average='micro', zero_division=0)
    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)
    f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)
    hamming = hamming_loss(labels, preds)
    
    # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
    precision, recall, f1_per_class, _ = precision_recall_fscore_support(
        labels, preds, average=None, zero_division=0
    )
    
    return {
        'f1_micro': f1_micro,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'hamming_loss': hamming,
        'precision_avg': precision.mean(),
        'recall_avg': recall.mean(),
    }


def compute_metrics_classification(pred):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–±—ã—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    
    accuracy = (preds == labels).mean()
    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)
    f1_weighted = f1_score(labels, preds, average='weighted', zero_division=0)
    
    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
    }


def train_topic_classifier(
    texts: List[str],
    topics_list: List[List[str]],
    output_dir: str = './trained_models/topic_classifier',
    model_name: str = 'cointegrated/rubert-tiny2',  # –õ–ï–ì–ö–û–í–ï–°–ù–ê–Ø –ú–û–î–ï–õ–¨!
    epochs: int = 8,
    batch_size: int = 32,  # –£–≤–µ–ª–∏—á–∏–ª–∏ batch size
    learning_rate: float = 3e-5
):
    """–û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–µ–º"""
    
    logger.info("=" * 60)
    logger.info("–û–ë–£–ß–ï–ù–ò–ï –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê –¢–ï–ú (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)")
    logger.info("=" * 60)
    logger.info(f"–ú–æ–¥–µ–ª—å: {model_name}")
    logger.info(f"–≠–ø–æ—Ö: {epochs}, Batch size: {batch_size}")
    
    # MultiLabelBinarizer –¥–ª—è —Ç–µ–º —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∫–ª–∞—Å—Å–æ–≤
    mlb = MultiLabelBinarizer(classes=EXTENDED_TOPICS)
    topic_labels = mlb.fit_transform(topics_list)
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º: {len(mlb.classes_)}")
    logger.info(f"–¢–µ–º—ã: {list(mlb.classes_)}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    topic_counts = topic_labels.sum(axis=0)
    for i, topic in enumerate(mlb.classes_):
        logger.info(f"  {topic}: {int(topic_counts[i])} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # Train/Val split
    X_train, X_val, y_train, y_val = train_test_split(
        texts, topic_labels, test_size=0.15, random_state=42
    )
    
    logger.info(f"Train samples: {len(X_train)}, Val samples: {len(X_val)}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=len(mlb.classes_),
        problem_type="multi_label_classification"
    )
    
    # –î–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = ReviewDataset(X_train, y_train, tokenizer, max_length=128)
    val_dataset = ReviewDataset(X_val, y_val, tokenizer, max_length=128)
    
    # –í—ã—á–∏—Å–ª—è–µ–º pos_weight –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤
    n_pos = topic_labels.sum(axis=0)
    N = topic_labels.shape[0]
    pos_weight = torch.tensor((N - n_pos) / np.clip(n_pos, 1, None), dtype=torch.float)
    logger.info(f"Pos weights (top-5): {pos_weight[:5].tolist()}")
    
    # Training arguments - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size * 2,  # –ë–æ–ª—å—à–∏–π batch –¥–ª—è eval
        learning_rate=learning_rate,
        warmup_ratio=0.1,  # 10% —à–∞–≥–æ–≤ warmup
        weight_decay=0.01,
        logging_dir=f'{output_dir}/logs',
        logging_steps=50,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1_weighted",
        greater_is_better=True,
        save_total_limit=2,
        report_to="none",
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=4,  # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        gradient_accumulation_steps=1,
    )
    
    # Trainer —Å pos_weight –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
    trainer = BCEWithPosWeightTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics_multilabel,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
        pos_weight=pos_weight
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    trainer.train()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {output_dir}")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ MultiLabelBinarizer
    mlb_path = Path(output_dir) / "mlb.pkl"
    with open(mlb_path, 'wb') as f:
        pickle.dump(mlb, f)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    logger.info("\n–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ:")
    eval_results = trainer.evaluate()
    logger.info(f"F1-micro: {eval_results['eval_f1_micro']:.4f}")
    logger.info(f"F1-macro: {eval_results['eval_f1_macro']:.4f}")
    logger.info(f"F1-weighted: {eval_results['eval_f1_weighted']:.4f}")
    logger.info(f"Hamming Loss: {eval_results['eval_hamming_loss']:.4f}")
    logger.info(f"Precision avg: {eval_results['eval_precision_avg']:.4f}")
    logger.info(f"Recall avg: {eval_results['eval_recall_avg']:.4f}")
    
    # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ per-class thresholds –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ F1
    logger.info("\n–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞...")
    val_logits = trainer.predict(val_dataset).predictions
    val_probs = 1 / (1 + np.exp(-val_logits))
    
    thresholds = []
    for c in range(val_probs.shape[1]):
        best_f1, best_t = 0.0, 0.40
        for t in np.linspace(0.25, 0.70, 30):
            preds_c = (val_probs[:, c] > t).astype(int)
            f1_c = f1_score(y_val[:, c], preds_c, zero_division=0)
            if f1_c > best_f1:
                best_f1, best_t = f1_c, t
        thresholds.append(float(best_t))
        logger.info(f"  {mlb.classes_[c]}: threshold={best_t:.3f}, F1={best_f1:.3f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤
    thresholds_path = Path(output_dir) / "thresholds.json"
    with open(thresholds_path, 'w', encoding='utf-8') as f:
        json.dump({
            "thresholds": thresholds,
            "classes": list(mlb.classes_)
        }, f, ensure_ascii=False, indent=2)
    logger.info(f"–ü–æ—Ä–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {thresholds_path}")
    
    logger.info("–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n")
    
    return model, tokenizer, mlb


def create_sentiment_dataset(texts: List[str], topics_list: List[List[str]], 
                              sentiments_list: List[List[str]], use_class_weights: bool = True):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è sentiment –∞–Ω–∞–ª–∏–∑–∞"""
    
    expanded_texts = []
    expanded_labels = []
    
    sentiment_to_id = {
        '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ': 0,
        '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ': 1,
        '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ': 2
    }
    
    for text, topics, sentiments in zip(texts, topics_list, sentiments_list):
        for topic, sentiment in zip(topics, sentiments):
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–º—É –∫–∞–∫ –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            augmented_text = f"[{topic}] {text}"
            expanded_texts.append(augmented_text)
            expanded_labels.append(sentiment_to_id[sentiment])
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(expanded_texts)} –ø–∞—Ä (—Ç–µ–∫—Å—Ç+—Ç–µ–º–∞ -> sentiment)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
    label_counts = Counter(expanded_labels)
    logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ sentiment:")
    for label_id, count in sorted(label_counts.items()):
        label_name = [k for k, v in sentiment_to_id.items() if v == label_id][0]
        percentage = count / len(expanded_labels) * 100
        logger.info(f"  {label_name}: {count} ({percentage:.1f}%)")
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
    class_weights = None
    if use_class_weights:
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(expanded_labels),
            y=expanded_labels
        )
        logger.info(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ sentiment (–¥–æ —É—Å–∏–ª–µ–Ω–∏—è): {class_weights}")
        
        # –£—Å–∏–ª–∏–≤–∞–µ–º –≤–µ—Å–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–∫–æ—Å–æ–º –≤ –ø–æ–∑–∏—Ç–∏–≤
        class_weights[0] *= 1.4  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ
        class_weights[1] *= 1.2  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ
        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
        
        logger.info(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ sentiment (–ø–æ—Å–ª–µ —É—Å–∏–ª–µ–Ω–∏—è): {class_weights}")
        class_weights = torch.tensor(class_weights, dtype=torch.float)
    
    return expanded_texts, expanded_labels, class_weights


def train_sentiment_classifier(
    texts: List[str],
    topics_list: List[List[str]],
    sentiments_list: List[List[str]],
    output_dir: str = './trained_models/sentiment_classifier',
    model_name: str = 'cointegrated/rubert-tiny2',  # –õ–ï–ì–ö–û–í–ï–°–ù–ê–Ø –ú–û–î–ï–õ–¨!
    epochs: int = 6,
    batch_size: int = 32,
    learning_rate: float = 3e-5
):
    """–û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    
    logger.info("=" * 60)
    logger.info("–û–ë–£–ß–ï–ù–ò–ï –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)")
    logger.info("=" * 60)
    logger.info(f"–ú–æ–¥–µ–ª—å: {model_name}")
    logger.info(f"–≠–ø–æ—Ö: {epochs}, Batch size: {batch_size}")
    
    # –°–æ–∑–¥–∞—ë–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    expanded_texts, expanded_labels, class_weights = create_sentiment_dataset(
        texts, topics_list, sentiments_list, use_class_weights=True
    )
    
    # Train/Val split —Å–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
    X_train, X_val, y_train, y_val = train_test_split(
        expanded_texts, expanded_labels, test_size=0.15, random_state=42, stratify=expanded_labels
    )
    
    logger.info(f"Train samples: {len(X_train)}, Val samples: {len(X_val)}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=3  # –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ, –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ
    )
    
    # –î–∞—Ç–∞—Å–µ—Ç—ã
    class SentimentDataset(Dataset):
        def __init__(self, texts, labels, tokenizer, max_length=128):
            self.texts = texts
            self.labels = labels
            self.tokenizer = tokenizer
            self.max_length = max_length
        
        def __len__(self):
            return len(self.texts)
        
        def __getitem__(self, idx):
            text = str(self.texts[idx])
            encoding = self.tokenizer(
                text,
                add_special_tokens=True,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            return {
                'input_ids': encoding['input_ids'].flatten(),
                'attention_mask': encoding['attention_mask'].flatten(),
                'labels': torch.tensor(self.labels[idx], dtype=torch.long)
            }
    
    train_dataset = SentimentDataset(X_train, y_train, tokenizer, max_length=128)
    val_dataset = SentimentDataset(X_val, y_val, tokenizer, max_length=128)
    
    # Training arguments - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size * 2,
        learning_rate=learning_rate,
        warmup_ratio=0.1,
        weight_decay=0.01,
        logging_dir=f'{output_dir}/logs',
        logging_steps=50,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1_weighted",
        greater_is_better=True,
        save_total_limit=2,
        report_to="none",
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=4,
    )
    
    # Trainer —Å class_weights –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π
    trainer = WeightedCELossTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics_classification,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
        class_weights=class_weights
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    trainer.train()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {output_dir}")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    logger.info("\n–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ:")
    eval_results = trainer.evaluate()
    logger.info(f"Accuracy: {eval_results['eval_accuracy']:.4f}")
    logger.info(f"F1-macro: {eval_results['eval_f1_macro']:.4f}")
    logger.info(f"F1-weighted: {eval_results['eval_f1_weighted']:.4f}")
    
    logger.info("–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n")
    
    return model, tokenizer


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    
    logger.info("\n" + "=" * 60)
    logger.info("OPTIMIZED ML TRAINING - –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å + –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
    logger.info("=" * 60 + "\n")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    if torch.cuda.is_available():
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    texts, topics_list, sentiments_list = load_data()
    
    # 2. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    analyze_data_distribution(topics_list, sentiments_list)
    
    # 3. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö...")
    texts, topics_list, sentiments_list = augment_data(texts, topics_list, sentiments_list)
    
    # 4. –û–±—É—á–µ–Ω–∏–µ topic –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (–ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –º–æ–¥–µ–ª—å)
    topic_model, topic_tokenizer, mlb = train_topic_classifier(
        texts=texts,
        topics_list=topics_list,
        output_dir='./trained_models/topic_classifier',
        model_name='cointegrated/rubert-tiny2',  # ~29MB –≤–º–µ—Å—Ç–æ 473MB!
        epochs=8,
        batch_size=32,
        learning_rate=3e-5
    )
    
    # 5. –û–±—É—á–µ–Ω–∏–µ sentiment –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (–ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –º–æ–¥–µ–ª—å)
    sentiment_model, sentiment_tokenizer = train_sentiment_classifier(
        texts=texts,
        topics_list=topics_list,
        sentiments_list=sentiments_list,
        output_dir='./trained_models/sentiment_classifier',
        model_name='cointegrated/rubert-tiny2',  # ~29MB –≤–º–µ—Å—Ç–æ 473MB!
        epochs=6,
        batch_size=32,
        learning_rate=3e-5
    )
    
    logger.info("\n" + "=" * 60)
    logger.info("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info("=" * 60)
    logger.info("–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
    logger.info("  - ./trained_models/topic_classifier/")
    logger.info("  - ./trained_models/sentiment_classifier/")
    logger.info("")
    logger.info("üìä –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò:")
    logger.info("  ‚Ä¢ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–µ–π: ~29MB –∫–∞–∂–¥–∞—è (–≤–º–µ—Å—Ç–æ 473MB)")
    logger.info("  ‚Ä¢ –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: 8-10x –Ω–∞ CPU")
    logger.info("  ‚Ä¢ –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: ~0.1-0.2 —Å–µ–∫ –Ω–∞ –æ—Ç–∑—ã–≤ (CPU)")
    logger.info("  ‚Ä¢ –ü–∞–º—è—Ç—å: ~200MB RAM –Ω–∞ –º–æ–¥–µ–ª—å")
    logger.info("")
    logger.info("üöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    logger.info("1. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª–∏: python model_optimized.py")
    logger.info("2. –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ Docker: docker compose build ml-service")
    logger.info("3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–∏—Å: docker compose up -d")
    logger.info("=" * 60 + "\n")


if __name__ == "__main__":
    main()
